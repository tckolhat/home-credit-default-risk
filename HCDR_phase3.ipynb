{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79740ca-5720-4d6f-9695-da235e5b73e0",
   "metadata": {},
   "source": [
    "# HCDR - Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e512c2-5239-44c8-8914-642d3f8093ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, precision_recall_curve, f1_score\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.calibration import calibration_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20a5e48-9e25-433c-a1b0-00cf29fa93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(in_path, name):\n",
    "    df = pd.read_csv(in_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_datasets(DATA_DIR, ds_names):\n",
    "    datasets = {}\n",
    "    for ds_name in ds_names:\n",
    "        datasets[ds_name] = load_data(os.path.join(\n",
    "            DATA_DIR, f'{ds_name}.csv'), ds_name)\n",
    "    return datasets\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f128c-5340-414c-8821-8ee556c8a125",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab960320-65ce-43c2-93cb-f50b3d4c9e36",
   "metadata": {},
   "source": [
    "### applications.csv\n",
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb6d291-8da4-4f04-a222-3d58df7f04d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_days(X):\n",
    "    mask = X > 0\n",
    "    X[mask] = np.NaN\n",
    "    # return np.log1p(-1*X)\n",
    "    return -X\n",
    "\n",
    "def preprocessing_transformations(df, inplace=False, impute_zero=()):\n",
    "    # pure state-less transformations\n",
    "    if inplace:\n",
    "        df_new = df\n",
    "    else:\n",
    "        df_new = df.copy()\n",
    "\n",
    "    right_skewed = ['AMT_ANNUITY']\n",
    "    left_skewed = []\n",
    "    days = ['DAYS_EMPLOYED']\n",
    "\n",
    "    def transform_left_skewed(X): return np.log(1+np.max(X)-X)\n",
    "\n",
    "    df_new[right_skewed] = np.log1p(df[right_skewed])\n",
    "    df_new[left_skewed] = transform_left_skewed(df[left_skewed])\n",
    "    df_new[days] = transform_days(df[days])\n",
    "\n",
    "    # others\n",
    "    df_new[impute_zero] = SimpleImputer(strategy='constant', fill_value=0).fit_transform(df_new[impute_zero])\n",
    "    df_new['NAME_FAMILY_STATUS'].replace('Unknown', np.nan, inplace=True)\n",
    "    df_new['ORGANIZATION_TYPE'].replace('XNA', np.nan, inplace=True)\n",
    "    df_new['CODE_GENDER'].replace('XNA', np.nan, inplace=True)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6bf86a-70e7-40fb-9e97-816ea985121e",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ec5d83f-7932-4163-b2c6-8e2202a1d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_features(df, inplace=False):\n",
    "    if inplace:\n",
    "        X = df\n",
    "    else:\n",
    "        X = df.copy()\n",
    "    X['annuity_income_percentage'] = X['AMT_ANNUITY'] / X['AMT_INCOME_TOTAL']\n",
    "    X['car_to_birth_ratio'] = X['OWN_CAR_AGE'] / X['DAYS_BIRTH']\n",
    "    X['car_to_employ_ratio'] = X['OWN_CAR_AGE'] / (1+X['DAYS_EMPLOYED'])\n",
    "    X['children_ratio'] = X['CNT_CHILDREN'] / X['CNT_FAM_MEMBERS']\n",
    "    X['credit_to_annuity_ratio'] = X['AMT_CREDIT'] / X['AMT_ANNUITY']\n",
    "    X['credit_to_goods_ratio'] = X['AMT_CREDIT'] / X['AMT_GOODS_PRICE']\n",
    "    X['credit_to_income_ratio'] = X['AMT_CREDIT'] / X['AMT_INCOME_TOTAL']\n",
    "    X['days_employed_percentage'] = X['DAYS_EMPLOYED'] / X['DAYS_BIRTH']\n",
    "    X['income_credit_percentage'] = X['AMT_INCOME_TOTAL'] / X['AMT_CREDIT']\n",
    "    X['income_per_child'] = X['AMT_INCOME_TOTAL'] / (1 + X['CNT_CHILDREN'])\n",
    "    X['income_per_person'] = X['AMT_INCOME_TOTAL'] / X['CNT_FAM_MEMBERS']\n",
    "    X['payment_rate'] = X['AMT_ANNUITY'] / X['AMT_CREDIT']\n",
    "    X['phone_to_birth_ratio'] = X['DAYS_LAST_PHONE_CHANGE'] / X['DAYS_BIRTH']\n",
    "    X['phone_to_employ_ratio'] = X['DAYS_LAST_PHONE_CHANGE'] / (1+X['DAYS_EMPLOYED'])\n",
    "    X['external_source_mean'] = X[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    X['cnt_non_child'] = X['CNT_FAM_MEMBERS'] - X['CNT_CHILDREN']\n",
    "    X['child_to_non_child_ratio'] = X['CNT_CHILDREN'] / X['cnt_non_child']\n",
    "    X['income_per_non_child'] = X['AMT_INCOME_TOTAL'] / X['cnt_non_child']\n",
    "    X['credit_per_person'] = X['AMT_CREDIT'] / X['CNT_FAM_MEMBERS']\n",
    "    X['credit_per_child'] = X['AMT_CREDIT'] / (1 + X['CNT_CHILDREN'])\n",
    "    X['credit_per_non_child'] = X['AMT_CREDIT'] / X['cnt_non_child']\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848ada0b-df9d-4e7c-ab6a-3a6bf0d5393e",
   "metadata": {},
   "source": [
    "### previous_applications.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d458be-31f9-4cd0-b49e-bf719c98879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prevAppsFeaturesAggregater(df, inplace=False):\n",
    "    # pure state-less transformations\n",
    "    if inplace:\n",
    "        df_new = df\n",
    "    else:\n",
    "        df_new = df.copy()\n",
    "\n",
    "    # Sorted df by decsion day\n",
    "    prev_applications_sorted = df_new.sort_values(\n",
    "        ['SK_ID_CURR', 'DAYS_DECISION'])\n",
    "\n",
    "    # Tranform days\n",
    "    days = ['DAYS_DECISION', 'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE',\n",
    "            'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION']\n",
    "    df_new[days] = transform_days(df[days])\n",
    "\n",
    "    aggr_df = pd.DataFrame({'SK_ID_CURR': df_new['SK_ID_CURR'].unique()})\n",
    "\n",
    "    # Compute min, max, min values\n",
    "    agg_ops = agg_ops = [\"min\", \"max\", \"mean\", \"sum\"]\n",
    "    features = [\n",
    "        'AMT_ANNUITY', 'AMT_APPLICATION', 'AMT_CREDIT', 'AMT_DOWN_PAYMENT', 'AMT_GOODS_PRICE', 'CNT_PAYMENT',\n",
    "        'HOUR_APPR_PROCESS_START', 'RATE_DOWN_PAYMENT', 'DAYS_DECISION', 'DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE',\n",
    "        'DAYS_LAST_DUE_1ST_VERSION', 'DAYS_LAST_DUE', 'DAYS_TERMINATION']\n",
    "    X = df_new.groupby([\"SK_ID_CURR\"], as_index=False).agg({ft: agg_ops for ft in features})\n",
    "    X.columns = X.columns.map(lambda col: '_'.join([x for x in col if x != '']))\n",
    "    aggr_df = aggr_df.merge(X, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Previous Application Count\n",
    "    prev_appl_count = df_new.groupby(by=['SK_ID_CURR'])['SK_ID_PREV'].nunique().reset_index()\n",
    "    prev_appl_count.rename(index=str, columns={'SK_ID_PREV': 'previous_applications_count'}, inplace=True)\n",
    "    aggr_df = aggr_df.merge(prev_appl_count, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Previous applications approved count\n",
    "    df_new['prev_applications_approved'] = (df_new['NAME_CONTRACT_STATUS'] == 'Approved').astype('int')\n",
    "    approved_count = df_new.groupby(by=['SK_ID_CURR'])['prev_applications_approved'].sum().reset_index()\n",
    "    aggr_df = aggr_df.merge(approved_count, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Previous applications refused count\n",
    "    df_new['prev_applications_refused'] = (df_new['NAME_CONTRACT_STATUS'] == 'Refused').astype('int')\n",
    "    refused_count = df_new.groupby(by=['SK_ID_CURR'])['prev_applications_refused'].sum().reset_index()\n",
    "    aggr_df = aggr_df.merge(refused_count, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # previous application invalid\n",
    "    df_new['prev_applications_invalid'] = (df_new['NAME_CONTRACT_STATUS'] == 'Canceled').astype(\n",
    "        'int') + (df_new['NAME_CONTRACT_STATUS'] == 'Unused offer').astype('int')\n",
    "    invalid_count = df_new.groupby(by=['SK_ID_CURR'])['prev_applications_invalid'].sum().reset_index()\n",
    "    aggr_df = aggr_df.merge(invalid_count, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Last application status(approved or rejected?)\n",
    "    prev_applications_sorted['prevAppl_last_approved'] = (\n",
    "        prev_applications_sorted['NAME_CONTRACT_STATUS'] == 'Approved').astype('int')\n",
    "    last_approved = prev_applications_sorted.groupby(by=['SK_ID_CURR'])['prevAppl_last_approved'].last().reset_index()\n",
    "    aggr_df = aggr_df.merge(last_approved, how='left', on=['SK_ID_CURR'])\n",
    "    return aggr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6e6ba-f06d-4dd7-bb93-447d37b0ea17",
   "metadata": {},
   "source": [
    "### POS_CASH_balance.csv\n",
    "#### Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c166ba30-28f4-40db-aba2-4f5fc85905ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cash_transform(cash, inplace=False):\n",
    "\n",
    "    cash['pos_cash_paid_late'] = (cash['SK_DPD'] > 0).astype(int)\n",
    "    cash['pos_cash_paid_late_with_tolerance'] = (cash['SK_DPD_DEF'] > 0).astype(int)\n",
    "\n",
    "    def fix_skew_months(X):\n",
    "        mask = X > 0\n",
    "        X[mask] = np.NaN\n",
    "        X = np.log(1+np.max(X)-X)\n",
    "        return -X\n",
    "\n",
    "    cash['MONTHS_BALANCE'] = fix_skew_months(cash['MONTHS_BALANCE'])\n",
    "    cash['CNT_INSTALMENT'] = np.log1p(cash['CNT_INSTALMENT'])\n",
    "    cash['CNT_INSTALMENT_FUTURE'] = np.log1p(cash['CNT_INSTALMENT_FUTURE'])\n",
    "\n",
    "    return cash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a452721f-80fd-4b4a-bc28-2f2135f87415",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f5087d-f3bf-4b0f-82b3-5b1449d00212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cashAppsFeaturesAggregater(df, inplace=False):\n",
    "    # pure state-less transformations\n",
    "    if inplace:\n",
    "        df_new = df\n",
    "    else:\n",
    "        df_new = df.copy()\n",
    "\n",
    "    aggr_df = pd.DataFrame({'SK_ID_CURR': df_new['SK_ID_CURR'].unique()})\n",
    "\n",
    "    agg_dict = {\n",
    "        'MONTHS_BALANCE': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'CNT_INSTALMENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'CNT_INSTALMENT_FUTURE': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'SK_DPD': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'SK_DPD_DEF': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'pos_cash_paid_late': [\"mean\"],\n",
    "        'pos_cash_paid_late_with_tolerance': [\"mean\"]\n",
    "    }\n",
    "\n",
    "    X = df_new.groupby([\"SK_ID_CURR\"], as_index=False).agg(agg_dict)\n",
    "    X.columns = X.columns.map(lambda col: '_'.join([x for x in col if x != '']))\n",
    "    aggr_df = aggr_df.merge(X, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    return aggr_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37947e68-0566-4101-8cfb-26d7d1cafbf5",
   "metadata": {},
   "source": [
    "### installments_payments.csv\n",
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e19e08-e602-49a2-9517-b554b82ce71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_transform(install, inplace=False):\n",
    "\n",
    "    install['installment_payment_diff'] = install['AMT_INSTALMENT'] - install['AMT_PAYMENT']\n",
    "    install['installment_paid_in_full'] = np.where(install['installment_payment_diff'] <= 0, 1,\n",
    "                                                   np.where(install['installment_payment_diff'] > 100.00, 0, 1))\n",
    "\n",
    "    install['installment_days_diff'] = install['DAYS_INSTALMENT'] - install['DAYS_ENTRY_PAYMENT']\n",
    "    install['installment_paid_in_time'] = np.where(install['installment_days_diff'] >= 0, 1, 0)\n",
    "\n",
    "    install['install_version'] = (install['NUM_INSTALMENT_VERSION'] > 0).astype(int)\n",
    "\n",
    "    def left_skew_days(X):\n",
    "        mask = X > 0\n",
    "        X[mask] = np.NaN\n",
    "        X = np.log(1+np.max(X)-X)\n",
    "        return -X\n",
    "\n",
    "    left_skewed = ['DAYS_INSTALMENT', 'DAYS_ENTRY_PAYMENT']\n",
    "    install[left_skewed] = left_skew_days(install[left_skewed])\n",
    "    install['NUM_INSTALMENT_NUMBER'] = np.log1p(install['NUM_INSTALMENT_NUMBER'])\n",
    "\n",
    "    return install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326eea2a-2f59-44e6-8dfb-496314990e2d",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a729546-542d-49ac-9918-dac33f776168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instlmntAppsFeaturesAggregater(df, inplace=False):\n",
    "    # pure state-less transformations\n",
    "    if inplace:\n",
    "        df_new = df\n",
    "    else:\n",
    "        df_new = df.copy()\n",
    "\n",
    "    aggr_df = pd.DataFrame({'SK_ID_CURR': df_new['SK_ID_CURR'].unique()})\n",
    "\n",
    "    # Compute min, max, min values\n",
    "    agg_dict = {\n",
    "        'NUM_INSTALMENT_VERSION': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'NUM_INSTALMENT_NUMBER': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'DAYS_INSTALMENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'DAYS_ENTRY_PAYMENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_INSTALMENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_PAYMENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'installment_payment_diff': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'installment_paid_in_full': [\"mean\"],\n",
    "        'installment_days_diff': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'installment_paid_in_time': [\"mean\"],\n",
    "        'install_version': [\"mean\"]\n",
    "    }\n",
    "    X = df_new.groupby([\"SK_ID_CURR\"], as_index=False).agg(agg_dict)\n",
    "    X.columns = X.columns.map(lambda col: '_'.join([x for x in col if x != '']))\n",
    "    aggr_df = aggr_df.merge(X, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    return aggr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e384e-2aee-462a-a5b7-ece87ec1fb88",
   "metadata": {},
   "source": [
    "### credit_card_balance.csv\n",
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c62719d0-90c9-48de-b4b8-1685b3444b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_transform(credit, inplace=False):\n",
    "\n",
    "    # # Amount used from limit\n",
    "    # credit['limit_use'] = credit['AMT_BALANCE'] / (1+credit['AMT_CREDIT_LIMIT_ACTUAL'])\n",
    "    # # Current payment / Min payment\n",
    "    # credit['payment_div_min'] = credit['AMT_PAYMENT_CURRENT'] / (1+credit['AMT_INST_MIN_REGULARITY'])\n",
    "    # # Late payment <-- 'CARD_IS_DPD'\n",
    "    # credit['late_payment'] = credit['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    # # How much drawing of limit\n",
    "    # credit['drawing_limit_ratio'] = credit['AMT_DRAWINGS_ATM_CURRENT'] / (1+credit['AMT_CREDIT_LIMIT_ACTUAL'])\n",
    "\n",
    "    def right_skew(X): return np.log1p(X)\n",
    "\n",
    "    right_skewed = ['AMT_BALANCE', 'AMT_CREDIT_LIMIT_ACTUAL', 'AMT_RECEIVABLE_PRINCIPAL', 'AMT_RECIVABLE',\n",
    "                    'AMT_TOTAL_RECEIVABLE', 'CNT_INSTALMENT_MATURE_CUM']\n",
    "    credit[right_skewed] = right_skew(credit[right_skewed])\n",
    "\n",
    "    return credit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2f0f7-b253-4a79-9f57-9da1cefcc9f3",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e21f7fc-f1b0-4921-986b-c47ff3790676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creditAppsFeaturesAggregater(df, inplace=False):\n",
    "    # pure state-less transformations\n",
    "    if inplace:\n",
    "        df_new = df\n",
    "    else:\n",
    "        df_new = df.copy()\n",
    "\n",
    "    aggr_df = pd.DataFrame({'SK_ID_CURR': df_new['SK_ID_CURR'].unique()})\n",
    "\n",
    "    # Compute min, max, min values\n",
    "    agg_dict = {\n",
    "        'AMT_BALANCE': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_DRAWINGS_ATM_CURRENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_DRAWINGS_CURRENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_DRAWINGS_OTHER_CURRENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_DRAWINGS_POS_CURRENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_INST_MIN_REGULARITY': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_PAYMENT_CURRENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_PAYMENT_TOTAL_CURRENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_RECEIVABLE_PRINCIPAL': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_RECIVABLE': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'AMT_TOTAL_RECEIVABLE': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'CNT_DRAWINGS_ATM_CURRENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'CNT_DRAWINGS_CURRENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'CNT_DRAWINGS_OTHER_CURRENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'CNT_DRAWINGS_POS_CURRENT': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        'CNT_INSTALMENT_MATURE_CUM': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        # 'limit_use': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        # 'payment_div_min': [\"min\", \"max\", \"mean\", \"sum\", \"var\"],\n",
    "        # 'late_payment': [\"mean\"],\n",
    "        # 'drawing_limit_ratio': [\"min\", \"max\", \"mean\", \"sum\", \"var\"]\n",
    "    }\n",
    "    X = df_new.groupby([\"SK_ID_CURR\"], as_index=False).agg(agg_dict)\n",
    "    X.columns = X.columns.map(lambda col: '_'.join([x for x in col if x != '']))\n",
    "    aggr_df = aggr_df.merge(X, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    return aggr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64cb08-2716-4f9d-a3b3-8c83857af223",
   "metadata": {},
   "source": [
    "### bureau.csv and bureau_balance.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c56693bb-debf-4554-984d-d01f56e35e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureauAppsFeaturesAggregater(df, inplace=False):\n",
    "    # pure state-less transformations\n",
    "    if inplace:\n",
    "        df_new = df\n",
    "    else:\n",
    "        df_new = df.copy()\n",
    "\n",
    "    aggr_df = pd.DataFrame({'SK_ID_CURR': df_new['SK_ID_CURR'].unique()})\n",
    "\n",
    "    # Compute min, max, min values\n",
    "    agg_ops = agg_ops = [\"min\", \"max\", \"mean\", \"sum\"]\n",
    "    features = ['AMT_CREDIT_SUM', 'DAYS_CREDIT', 'DAYS_CREDIT_UPDATE', 'DAYS_CREDIT_ENDDATE']\n",
    "    X = df_new.groupby([\"SK_ID_CURR\"], as_index=False).agg({ft: agg_ops for ft in features})\n",
    "    X.columns = X.columns.map(lambda col: '_'.join([x for x in col if x != '']))\n",
    "    aggr_df = aggr_df.merge(X, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    return aggr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab905ac-deb4-4b63-831e-1082fce3a2c0",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57727b5c-aa94-4f21-a259-5ac64b50b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prep_pipeline(num_selected=None, cat_selected=None):\n",
    "    num_pipeline = Pipeline([\n",
    "        ('new_features', FunctionTransformer(add_new_features)),\n",
    "        ('selector', DataFrameSelector(num_selected)),\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "    cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_selected)),\n",
    "        #('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "    ])\n",
    "\n",
    "    data_prep_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline)\n",
    "    ])\n",
    "    return data_prep_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01e1dd-b97f-4503-b369-2984977de400",
   "metadata": {},
   "source": [
    "### Load, Preprocess and Aggregate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db36b00a-e030-4f8f-bdc3-0bdb729be699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_data():\n",
    "    # load data\n",
    "    DATA_DIR = \"../data\"\n",
    "    # ds_names = (\"application_train\", \"application_test\", \"bureau\",\"bureau_balance\",\"credit_card_balance\",\"installments_payments\",\n",
    "    #             \"previous_application\",\"POS_CASH_balance\")\n",
    "    ds_names = (\"application_train\", \"application_test\", \"bureau\", \"credit_card_balance\", \"installments_payments\",\n",
    "                \"previous_application\", \"POS_CASH_balance\")\n",
    "    datasets = load_datasets(DATA_DIR, ds_names)\n",
    "    print('loaded data')\n",
    "\n",
    "    # Preparing data\n",
    "    appl_train = datasets['application_train']\n",
    "    prevData_aggr = prevAppsFeaturesAggregater(datasets['previous_application'])\n",
    "\n",
    "    # bureau\n",
    "    bureauData_aggr = bureauAppsFeaturesAggregater(datasets['bureau'])\n",
    "    data_aggr = appl_train.merge(prevData_aggr, how='left', on=['SK_ID_CURR'])\n",
    "    data_aggr = data_aggr.merge(bureauData_aggr, how='left', on=['SK_ID_CURR'])\n",
    "\n",
    "    # cash\n",
    "    cash = datasets['POS_CASH_balance']\n",
    "    cashData_aggr = cashAppsFeaturesAggregater(cash_transform(cash))\n",
    "    data_aggr = data_aggr.merge(cashData_aggr, how='left', on=['SK_ID_CURR'])\n",
    "    install = datasets['installments_payments']\n",
    "    instlmntData_aggr = instlmntAppsFeaturesAggregater(install_transform(install))\n",
    "    data_aggr = data_aggr.merge(instlmntData_aggr, how='left', on=['SK_ID_CURR'])\n",
    "    credit = datasets['credit_card_balance']\n",
    "    creditData_aggr = creditAppsFeaturesAggregater(credit_transform(credit))\n",
    "\n",
    "    data_aggr = data_aggr.merge(creditData_aggr, how='left', on=['SK_ID_CURR'])\n",
    "    impute_zero = ['OWN_CAR_AGE', 'previous_applications_count', 'prev_applications_approved',\n",
    "                   'prev_applications_refused', 'prev_applications_invalid', 'prevAppl_last_approved']\n",
    "    processed_data = preprocessing_transformations(data_aggr, impute_zero=impute_zero)\n",
    "    \n",
    "    \n",
    "    # test data preprocessing\n",
    "    \n",
    "    app_test = datasets['application_test']\n",
    "    app_test_aggr = app_test.merge(prevData_aggr, how='left', on=['SK_ID_CURR'])\n",
    "    app_test_aggr = app_test_aggr.merge(bureauData_aggr, how='left', on=['SK_ID_CURR'])\n",
    "    app_test_aggr = app_test_aggr.merge(cashData_aggr, how='left', on=['SK_ID_CURR'])\n",
    "    app_test_aggr = app_test_aggr.merge(instlmntData_aggr, how='left', on=['SK_ID_CURR'])\n",
    "    app_test_aggr = app_test_aggr.merge(creditData_aggr, how='left', on=['SK_ID_CURR'])\n",
    "    processed_test_data = preprocessing_transformations(app_test_aggr, impute_zero=impute_zero)\n",
    "\n",
    "    # training\n",
    "\n",
    "    app_num_attribs = ['AMT_INCOME_TOTAL', 'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'AMT_CREDIT', 'REGION_POPULATION_RELATIVE',\n",
    "                       'DAYS_EMPLOYED', 'DAYS_BIRTH', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_ID_PUBLISH',\n",
    "                       'DAYS_REGISTRATION', 'DAYS_LAST_PHONE_CHANGE', 'OWN_CAR_AGE', 'OBS_30_CNT_SOCIAL_CIRCLE',\n",
    "                       'DEF_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE', 'DEF_60_CNT_SOCIAL_CIRCLE',\n",
    "                       'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "                       'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR',\n",
    "                       'CNT_CHILDREN', 'CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY',\n",
    "                       'HOUR_APPR_PROCESS_START']\n",
    "    new_app_attribs = [\n",
    "        'annuity_income_percentage', 'car_to_birth_ratio', 'car_to_employ_ratio', 'children_ratio',\n",
    "        'credit_to_annuity_ratio', 'credit_to_goods_ratio', 'credit_to_income_ratio', 'days_employed_percentage',\n",
    "        'income_credit_percentage', 'income_per_child', 'income_per_person', 'payment_rate', 'phone_to_birth_ratio',\n",
    "        'phone_to_employ_ratio', 'external_source_mean', 'cnt_non_child', 'child_to_non_child_ratio',\n",
    "        'income_per_non_child', 'credit_per_person', 'credit_per_child', 'credit_per_non_child']\n",
    "    prev_aggr_attribs = prevData_aggr.columns.to_list()\n",
    "    bureau_aggr_attribs = bureauData_aggr.columns.to_list()\n",
    "    cash_columns = cashData_aggr.columns.to_list()\n",
    "    install_columns = instlmntData_aggr.columns.to_list()\n",
    "    credit_columns = creditData_aggr.columns.to_list()\n",
    "\n",
    "    app_cat_attribs = [\n",
    "        'NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE',\n",
    "        'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE', 'HOUSETYPE_MODE',\n",
    "        'WALLSMATERIAL_MODE', 'EMERGENCYSTATE_MODE', 'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE', 'FLAG_MOBIL',\n",
    "        'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL',\n",
    "        'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION',\n",
    "        'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY', 'FLAG_DOCUMENT_2',\n",
    "        'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', 'FLAG_DOCUMENT_7',\n",
    "        'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',\n",
    "        'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17',\n",
    "        'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']\n",
    "\n",
    "    num_attribs = app_num_attribs + new_app_attribs + prev_aggr_attribs + bureau_aggr_attribs + cash_columns + install_columns + credit_columns\n",
    "\n",
    "    cat_attribs = app_cat_attribs\n",
    "\n",
    "    return (processed_data, processed_test_data, num_attribs, cat_attribs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8584c0d4-2e21-471d-85ff-d571930f9b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nl/pxf72qk574l1rzzh347l6x2w0000gn/T/ipykernel_73004/1209287635.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[mask] = np.NaN\n",
      "/Users/deepakduggirala/miniforge3/envs/tf_m1/lib/python3.8/site-packages/pandas/core/frame.py:3718: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._where(-key, value, inplace=True)\n",
      "/var/folders/nl/pxf72qk574l1rzzh347l6x2w0000gn/T/ipykernel_73004/3953342808.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[mask] = np.NaN\n",
      "/var/folders/nl/pxf72qk574l1rzzh347l6x2w0000gn/T/ipykernel_73004/3002739713.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[mask] = np.NaN\n",
      "/var/folders/nl/pxf72qk574l1rzzh347l6x2w0000gn/T/ipykernel_73004/1209287635.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[mask] = np.NaN\n",
      "/Users/deepakduggirala/miniforge3/envs/tf_m1/lib/python3.8/site-packages/pandas/core/frame.py:3718: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._where(-key, value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "processed_data, processed_test_data, num_attribs, cat_attribs = load_process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f4f94a9-1d5f-4bde-8bf8-c1e8c2080759",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = processed_data['TARGET']\n",
    "X = processed_data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "data_prep_pipeline = make_prep_pipeline(num_attribs, cat_attribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7773ff1-dde5-4388-a3dc-7b0d82b99dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((184506, 354), (61502, 354), (61503, 354))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d782d2e-a90b-4432-888d-78d1e7b06e66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac18d4df-ebe4-4db1-adf3-a6a3a66bc7e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegressionCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegressionCV' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(42)\n",
    "pipeline_with_selector = Pipeline([\n",
    "    (\"preparation\", data_prep_pipeline),\n",
    "    (\"feature_selector\", SelectFromModel(LogisticRegressionCV(\n",
    "        C=np.logspace(-4, -1, 32),\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=0))),\n",
    "])\n",
    "\n",
    "# _ = pipeline_with_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f759ec35-cf58-44bf-9187-ba8f2fb3fe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 1.3 s, total: 32.4 s\n",
      "Wall time: 32.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(42)\n",
    "pipeline_with_selector = Pipeline([\n",
    "    (\"preparation\", data_prep_pipeline),\n",
    "    (\"feature_selector\", SelectFromModel(LogisticRegression(\n",
    "        C=0.006,\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=0))),\n",
    "])\n",
    "\n",
    "_ = pipeline_with_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d99983b-68bd-408d-aa1c-63a57a988043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: 491, num_attribs: 287, cat_features: 204\n",
      "attribs: 334, features: 491, selected_features=173\n"
     ]
    }
   ],
   "source": [
    "cat_pipeline = data_prep_pipeline.transformer_list[1][1]\n",
    "cat_features = [f'{base}_{c}'for base, ohe_c in zip(\n",
    "    cat_attribs, cat_pipeline.named_steps['ohe'].categories_) for c in ohe_c]\n",
    "features = num_attribs + cat_features\n",
    "print(f'features: {len(features)}, num_attribs: {len(num_attribs)}, cat_features: {len(cat_features)}')\n",
    "\n",
    "selector_model = pipeline_with_selector.named_steps['feature_selector']\n",
    "selected_features = list(np.array(features)[selector_model.get_support()])\n",
    "print(f'attribs: {len(num_attribs + cat_attribs)}, features: {len(features)}, selected_features={len(selected_features)}')\n",
    "\n",
    "selected_attribs = set([f if f in num_attribs else '_'.join(f.split('_')[:-1]) for f in selected_features])\n",
    "unused_attribs = set(num_attribs+cat_attribs) - selected_attribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c917b192-61ab-4d19-b82b-8e7fbd8e775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_attribs = set([f if f in num_attribs else '_'.join(f.split('_')[:-1]) for f in selected_features])\n",
    "unused_attribs = set(num_attribs+cat_attribs) - selected_attribs\n",
    "\n",
    "# print('\\n\\n\\nselected')\n",
    "# print(selected_attribs)\n",
    "\n",
    "# print('\\n\\n\\nunused')\n",
    "# print(unused_attribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79532de4-493f-4e91-81c6-c168c5bc7034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((184506, 173), (61502, 173), (61503, 173))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_xfm = pipeline_with_selector.transform(X_train)\n",
    "X_valid_xfm = pipeline_with_selector.transform(X_valid)\n",
    "X_test_xfm = pipeline_with_selector.transform(X_test)\n",
    "X_train_xfm.shape, X_valid_xfm.shape, X_test_xfm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb3f3ca1-b8d0-4bb9-8a44-5dd5a9d685f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <td>0.036947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLAG_OWN_CAR_Y</th>\n",
       "      <td>0.035180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CODE_GENDER_F</th>\n",
       "      <td>0.034490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>external_source_mean</th>\n",
       "      <td>0.031756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <td>0.029243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMT_INST_MIN_REGULARITY_var</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMT_INST_MIN_REGULARITY_min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMT_DRAWINGS_POS_CURRENT_sum</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMT_DRAWINGS_POS_CURRENT_mean</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FLAG_DOCUMENT_21_1</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>491 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Imp\n",
       "EXT_SOURCE_3                   0.036947\n",
       "FLAG_OWN_CAR_Y                 0.035180\n",
       "CODE_GENDER_F                  0.034490\n",
       "external_source_mean           0.031756\n",
       "EXT_SOURCE_2                   0.029243\n",
       "...                                 ...\n",
       "AMT_INST_MIN_REGULARITY_var    0.000000\n",
       "AMT_INST_MIN_REGULARITY_min    0.000000\n",
       "AMT_DRAWINGS_POS_CURRENT_sum   0.000000\n",
       "AMT_DRAWINGS_POS_CURRENT_mean  0.000000\n",
       "FLAG_DOCUMENT_21_1             0.000000\n",
       "\n",
       "[491 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = pd.DataFrame(selector_model.estimator_.coef_.T/np.sum(np.abs(selector_model.estimator_.coef_)), \n",
    "                           index=features,\n",
    "                           columns=['Imp']).abs().sort_values(by='Imp', ascending=False)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1271ab39-6a24-4065-a4d7-ea8853604586",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances.to_csv('importances.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2dd546-0a33-4d77-9c81-1423e3b700a0",
   "metadata": {},
   "source": [
    "## Models Explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3467f5cb-054b-475b-9ae8-aabb48529bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del expLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "374bd851-90fa-46c6-a252-0365f30231da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "try:\n",
    "    expLog\n",
    "except NameError:\n",
    "    expLog = pd.DataFrame(columns=[\"exp_name\", \n",
    "                                   \"Train Acc\", \n",
    "                                   \"Valid Acc\",\n",
    "                                   \"Test  Acc\",\n",
    "                                   \"Train AUC\", \n",
    "                                   \"Valid AUC\",\n",
    "                                   \"Test  AUC\"\n",
    "                                  ])\n",
    "def log_exp(res, name):\n",
    "    y_train_pred_proba, y_valid_pred_proba, y_test_pred_proba = res\n",
    "    y_train_pred = y_train_pred_proba > 0.5\n",
    "    y_valid_pred = y_valid_pred_proba > 0.5\n",
    "    y_test_pred = y_test_pred_proba > 0.5\n",
    "\n",
    "    exp_name = f\"{name}_{len(selected_features)}_features\"\n",
    "    expLog.loc[len(expLog)] = [f\"{exp_name}\"] + list(np.round(\n",
    "                   [accuracy_score(y_train, y_train_pred), \n",
    "                    accuracy_score(y_valid, y_valid_pred),\n",
    "                    accuracy_score(y_test, y_test_pred),\n",
    "                    roc_auc_score(y_train, y_train_pred_proba),\n",
    "                    roc_auc_score(y_valid, y_valid_pred_proba),\n",
    "                    roc_auc_score(y_test, y_test_pred_proba)],\n",
    "        4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95ddca21-9f0c-478a-b9a6-19b605824939",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced7009-544b-4b2d-aa2c-eb07de69b41a",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ff63e18-f02b-4729-911f-2366c5ba110c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 s, sys: 2.91 s, total: 22.5 s\n",
      "Wall time: 3.24 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', max_iter=1000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train_xfm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1eb1fdfc-7750-41e8-ba41-187b690a9fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Valid Acc</th>\n",
       "      <th>Test  Acc</th>\n",
       "      <th>Train AUC</th>\n",
       "      <th>Valid AUC</th>\n",
       "      <th>Test  AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_reg_baseline_173_features</td>\n",
       "      <td>0.7074</td>\n",
       "      <td>0.7053</td>\n",
       "      <td>0.7061</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.7716</td>\n",
       "      <td>0.7703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             exp_name  Train Acc  Valid Acc  Test  Acc  \\\n",
       "0  logistic_reg_baseline_173_features     0.7074     0.7053     0.7061   \n",
       "\n",
       "   Train AUC  Valid AUC  Test  AUC  \n",
       "0      0.773     0.7716     0.7703  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred_proba = model.predict_proba(X_train_xfm)[:, 1]\n",
    "y_valid_pred_proba = model.predict_proba(X_valid_xfm)[:, 1]\n",
    "y_test_pred_proba = model.predict_proba(X_test_xfm)[:, 1]\n",
    "baseline_results['lr'] = (y_train_pred_proba, y_valid_pred_proba, y_test_pred_proba)\n",
    "\n",
    "log_exp(baseline_results['lr'], 'logistic_reg_baseline')\n",
    "expLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77471a4-1b8a-4746-89e8-32b9f047a039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079fd145-943c-4000-804f-1c0148b15d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "673d79ae-e479-44fd-b847-bbd5fbfb9388",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "21356079-8009-460c-819e-75edfbf1c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "c2d50647-00db-4207-93c1-62c3cddd6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train_xfm.astype(np.float32))\n",
    "X_validation_tensor = torch.from_numpy(X_valid_xfm.astype(np.float32))\n",
    "X_test_tensor = torch.from_numpy(X_test_xfm.astype(np.float32))\n",
    "y_train_tensor = torch.from_numpy(y_train.astype(np.float32).values)\n",
    "y_test_tensor = torch.from_numpy(y_test.astype(np.float32).values)\n",
    "y_validation_tensor = torch.from_numpy(y_valid.astype(np.float32).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "826cf7ab-c544-4a88-9b42-709d293b9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TensorDataset in PyTorch\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valid_dataset = torch.utils.data.TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "4f45184c-04cf-4bb3-8ed5-1c4eb26bbc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "batch_size = 96\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=X_valid_xfm.shape[0], shuffle=False, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=X_test_xfm.shape[0], shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "aae3fd84-08dc-4735-8ba4-ff173de10e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter(\"runs/HCDR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "f35982a1-b23c-4dda-8d39-164b77abd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HCDRNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(hidden_sizes[2], num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "cc3dd9ba-06d1-446e-a691-2abfc056f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model\n",
    "net = HCDRNet(173, (512, 256, 128), 1)\n",
    "\n",
    "# Out loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Our optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, nesterov=True, momentum=0.9, dampening=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "a5be70cb-6a19-44bb-956d-0e5239be84d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1          [-1, 184506, 512]          89,088\n",
      "              ReLU-2          [-1, 184506, 512]               0\n",
      "            Linear-3          [-1, 184506, 256]         131,328\n",
      "              ReLU-4          [-1, 184506, 256]               0\n",
      "            Linear-5          [-1, 184506, 128]          32,896\n",
      "              ReLU-6          [-1, 184506, 128]               0\n",
      "            Linear-7            [-1, 184506, 1]             129\n",
      "================================================================\n",
      "Total params: 253,441\n",
      "Trainable params: 253,441\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 121.76\n",
      "Forward/backward pass size (MB): 2523.95\n",
      "Params size (MB): 0.97\n",
      "Estimated Total Size (MB): 2646.68\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary \n",
    "summary(net, X_train_xfm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "15cc59fe-b27f-4a1f-a077-e0cf313a2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "items, classes = next(iter(trainloader))\n",
    "writer.add_graph(net, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "e1a0aa73-0604-43b3-a5c2-32ed2077cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, outputs):\n",
    "    sig = nn.Sigmoid()\n",
    "    out_tensors = sig(outputs)\n",
    "    y_test_pred_proba = out_tensors.detach().numpy()\n",
    "    return accuracy_score(y_true, y_test_pred_proba>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "1d0aa383-426c-49fc-8952-b7aa2e2b6166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rocauc(y_true, outputs):\n",
    "    \n",
    "    return roc_auc_score(y_true, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "55409406-d9ae-498f-9272-320925fe889b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 0.2948, Validation Loss: 0.2555, Train ROCAUC: 0.6223, Validation ROCAUC: 0.7343\n",
      "Epoch 2/15, Train Loss: 0.2513, Validation Loss: 0.2473, Train ROCAUC: 0.7475, Validation ROCAUC: 0.7578\n",
      "Epoch 3/15, Train Loss: 0.2469, Validation Loss: 0.2452, Train ROCAUC: 0.7609, Validation ROCAUC: 0.7638\n",
      "Epoch 4/15, Train Loss: 0.2450, Validation Loss: 0.2442, Train ROCAUC: 0.7663, Validation ROCAUC: 0.7670\n",
      "Epoch 5/15, Train Loss: 0.2438, Validation Loss: 0.2435, Train ROCAUC: 0.7698, Validation ROCAUC: 0.7691\n",
      "Epoch 6/15, Train Loss: 0.2429, Validation Loss: 0.2433, Train ROCAUC: 0.7724, Validation ROCAUC: 0.7698\n",
      "Epoch 7/15, Train Loss: 0.2421, Validation Loss: 0.2427, Train ROCAUC: 0.7746, Validation ROCAUC: 0.7711\n",
      "Epoch 8/15, Train Loss: 0.2414, Validation Loss: 0.2427, Train ROCAUC: 0.7763, Validation ROCAUC: 0.7713\n",
      "Epoch 9/15, Train Loss: 0.2408, Validation Loss: 0.2425, Train ROCAUC: 0.7781, Validation ROCAUC: 0.7716\n",
      "Epoch 10/15, Train Loss: 0.2402, Validation Loss: 0.2423, Train ROCAUC: 0.7798, Validation ROCAUC: 0.7723\n",
      "Epoch 11/15, Train Loss: 0.2396, Validation Loss: 0.2424, Train ROCAUC: 0.7813, Validation ROCAUC: 0.7721\n",
      "Epoch 12/15, Train Loss: 0.2390, Validation Loss: 0.2421, Train ROCAUC: 0.7829, Validation ROCAUC: 0.7730\n",
      "Epoch 13/15, Train Loss: 0.2384, Validation Loss: 0.2422, Train ROCAUC: 0.7845, Validation ROCAUC: 0.7727\n",
      "Epoch 14/15, Train Loss: 0.2378, Validation Loss: 0.2419, Train ROCAUC: 0.7861, Validation ROCAUC: 0.7731\n",
      "Epoch 15/15, Train Loss: 0.2373, Validation Loss: 0.2420, Train ROCAUC: 0.7875, Validation ROCAUC: 0.7733\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_accuracy = [0]\n",
    "valid_accuracy = []\n",
    "train_rocauc = [0]\n",
    "valid_rocauc = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss = 0\n",
    "    num_iter = 0\n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    \n",
    "    for i, (items, classes) in enumerate(trainloader):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        items = Variable(items)\n",
    "        classes = Variable(classes)\n",
    "        \n",
    "        net.train()           # Put the network into training mode\n",
    "        \n",
    "        optimizer.zero_grad() # Clear off the gradients from any past operation\n",
    "        outputs = net(items)  # Do the forward pass\n",
    "        loss = criterion(outputs, classes.unsqueeze(1)) # Calculate the loss\n",
    "        loss.backward()       # Calculate the gradients with help of back propagation\n",
    "        optimizer.step()      # Ask the optimizer to adjust the parameters based on the gradients\n",
    "        \n",
    "        running_loss += loss.detach().item()\n",
    "        num_iter += 1\n",
    "        y_true = np.hstack((y_true, classes.detach().numpy()))\n",
    "        y_pred = np.hstack((y_pred, outputs.detach().numpy().ravel()))\n",
    "\n",
    "    net.eval()                 # Put the network into evaluation mode\n",
    "    \n",
    "    # Book keeping\n",
    "    # Record the loss\n",
    "    train_loss.append(running_loss/num_iter)\n",
    "\n",
    "    # What was our train accuracy?\n",
    "    train_accuracy.append(accuracy_score(y_true, expit(y_pred)>0.5))\n",
    "    train_rocauc.append(roc_auc_score(y_true, expit(y_pred)))\n",
    "    \n",
    "    # How did we do on the test set (the unseen set)\n",
    "    # Record the correct predictions for test data\n",
    "    \n",
    "    X_valid_tensor, y_valid_tensor = valid_dataset.tensors\n",
    "    \n",
    "    test_items = torch.FloatTensor(X_valid_tensor)\n",
    "    test_classes = torch.FloatTensor(y_valid_tensor)\n",
    "\n",
    "    outputs = net(Variable(test_items))\n",
    "    loss = criterion(outputs, Variable(test_classes.unsqueeze(1)))\n",
    "    valid_loss.append(loss.data)\n",
    "    valid_accuracy.append(accuracy(test_classes.detach().numpy(), outputs))\n",
    "    valid_rocauc.append(rocauc(test_classes.detach().numpy(), outputs))\n",
    "    \n",
    "    \n",
    "    print ('Epoch %d/%d, Train Loss: %.4f, Validation Loss: %.4f, Train ROCAUC: %.4f, Validation ROCAUC: %.4f' \n",
    "       %(epoch+1, num_epochs, train_loss[-1], valid_loss[-1], train_rocauc[-1], valid_rocauc[-1]))\n",
    "    \n",
    "    writer.add_scalar('Loss/train', train_loss[-1], epoch+1)\n",
    "    writer.add_scalar('Loss/test', valid_loss[-1], epoch+1)\n",
    "    writer.add_scalar('Accuracy/train', train_accuracy[-1], epoch+1)\n",
    "    writer.add_scalar('Accuracy/test', valid_accuracy[-1], epoch+1)\n",
    "    writer.add_scalar('ROCAUC/train', train_rocauc[-1], epoch+1)\n",
    "    writer.add_scalar('ROCAUC/test', valid_rocauc[-1], epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "ab9669b8-eca3-40df-bbf4-212836d154a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "1c07c02d-69d1-4cdf-badd-2e7e2732315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor, y_test_tensor = test_dataset.tensors\n",
    "\n",
    "test_items = torch.FloatTensor(X_test_tensor)\n",
    "test_classes = torch.FloatTensor(y_test_tensor)\n",
    "\n",
    "net.eval()\n",
    "outputs = net(Variable(test_items))\n",
    "test_acc = accuracy(y_test, outputs)\n",
    "test_rocauc = rocauc(y_test, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64122039-840e-4b4a-bce6-c538a1608ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_name</th>\n",
       "      <th>Train Acc</th>\n",
       "      <th>Valid Acc</th>\n",
       "      <th>Test  Acc</th>\n",
       "      <th>Train AUC</th>\n",
       "      <th>Valid AUC</th>\n",
       "      <th>Test  AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic_reg_baseline_173_features</td>\n",
       "      <td>0.7074</td>\n",
       "      <td>0.7053</td>\n",
       "      <td>0.7061</td>\n",
       "      <td>0.7730</td>\n",
       "      <td>0.7716</td>\n",
       "      <td>0.7703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP_173_features</td>\n",
       "      <td>0.9193</td>\n",
       "      <td>0.9197</td>\n",
       "      <td>0.9198</td>\n",
       "      <td>0.7875</td>\n",
       "      <td>0.7733</td>\n",
       "      <td>0.7727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             exp_name  Train Acc  Valid Acc  Test  Acc  \\\n",
       "0  logistic_reg_baseline_173_features     0.7074     0.7053     0.7061   \n",
       "1                    MLP_173_features     0.9193     0.9197     0.9198   \n",
       "\n",
       "   Train AUC  Valid AUC  Test  AUC  \n",
       "0     0.7730     0.7716     0.7703  \n",
       "1     0.7875     0.7733     0.7727  "
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name='MLP'\n",
    "exp_name = f\"{name}_{len(selected_features)}_features\"\n",
    "expLog.loc[len(expLog)] = [f\"{exp_name}\"] + list(np.round(\n",
    "               [train_accuracy[-1], valid_accuracy[-1].item(), test_acc, train_rocauc[-1], valid_rocauc[-1], test_rocauc],\n",
    "    4))\n",
    "expLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c344fea-97b7-4b53-8448-00061203ffe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9805963-6e2a-49fe-ae47-f3ad3444fdda",
   "metadata": {},
   "source": [
    "## Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "7fea9673-4352-49a8-a336-19643b68b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kaggle_test = pipeline_with_selector.transform(processed_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "a79ce17c-4191-4ed6-b5b3-f3047b75abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kaggle_tensor = torch.from_numpy(X_kaggle_test.astype(np.float32))\n",
    "outputs = net(X_kaggle_tensor)\n",
    "sig = nn.Sigmoid()\n",
    "out_tensors = sig(outputs)\n",
    "test_class_scores = out_tensors.detach().numpy()\n",
    "\n",
    "name = 'MLP'\n",
    "submit_df = processed_test_data[['SK_ID_CURR']].copy()\n",
    "submit_df['TARGET'] = test_class_scores\n",
    "submit_df.to_csv(f'{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "ebd2709d-be9b-4b6e-b113-7e728566a63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: kaggle\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c home-credit-default-risk -f MLP.csv -m \"MLP\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ac235-2194-41fe-a82d-2dd27cf26bda",
   "metadata": {},
   "source": [
    "![mlp_kaggle](images/mlp_kaggle.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
